# ===== Snakemake rules for running the 10x Cell Ranger pipeline ===============

import os 
import re
import glob

def _find_experiment_type(wildcards):
    # Check experiment type so the copying can be done correctly
    if SAMPLE_DICT_VDJ_T[wildcards.sample] and SAMPLE_DICT_VDJ_B[wildcards.sample]:
        return("VDJT_VDJB")
    elif SAMPLE_DICT_VDJ_T[wildcards.sample]:
        return("VDJT")
    elif SAMPLE_DICT_VDJ_B[wildcards.sample]:
        return("VDJB")
    elif SAMPLE_DICT_ADT[wildcards.sample]:
        retrun("ADT")
    else:
        return("RNA")

def _find_run_type(wildcards):
    if SAMPLE_DICT_VDJ_T[wildcards.sample] or SAMPLE_DICT_VDJ_B[wildcards.sample]:
        reuturn("VDJ")
        # Run cellranger count for CITE-seq and gene expression
    elif SAMPLE_DICT_ADT[wildcards.sample]:
        return("CITE")
    else:
        return("Count")

# Merge fastqs and create symlinks =============================================
# This rule will either merge multiple fastqs into a single file or create a 
# symlink to the original fastq. If a comma separated list of sample names is 
# provided all fastqs that begin with either name will be merged into one file 
# that has a new name. If only a single name is provided a symlink will be 
# created for each fastq that begins with the name.

rule merge_fastqs:
    output:
        "{results}/snake_outs/{sample}_merge_fastqs_done.txt"
    params:
        raw_data   = RAW_DATA_DICT,
        fq_dir     = FASTQ_DIR,
        fq_info    = FASTQ_INFO,
        rna_dict   = SAMPLE_DICT_RNA,
        adt_dict   = SAMPLE_DICT_ADT,
        vdj_t_dict = SAMPLE_DICT_VDJ_T,
        vdj_b_dict = SAMPLE_DICT_VDJ_B
    resources:
        slurm_extra=lambda wildcards: (
            f"--output={wildcards.results}/logs/merge_fastq/{wildcards.sample}_fastq.out "
            f"--error={wildcards.results}/logs/merge_fastq/{wildcards.sample}_fastq.err"
        )
    threads:
        1   
    run:
        # Function to retrieve fastq paths
        def _get_fq_paths(sample, read, raw_dir):
            path_list = []
            raw_dir = raw_dir.split(",")
            for directory in raw_dir:
                _check_path(directory)
                fq_paths = os.path.join(directory, sample + "*" + read + "*.fastq.gz")
                fq_paths = glob.glob(os.path.abspath(fq_paths))
                if fq_paths:
                    print(fq_paths)
                    if re.search(sample + "\.*_[0-9]" + FASTQ_INFO, fq_paths[0]):
                        path_dict = {}
                        fastq_count = 0
                        for x in fq_paths:
                            fastq_num = re.search("_[0-9]" + FASTQ_INFO, x).group(0)[1]
                            path_dict[int(fastq_num) - 1] = fastq_count
                            fastq_count += 1
                        [path_list.append(fq_paths[path_dict[i]]) for i in range(len(path_dict))]
                    else:
                        [path_list.append(x) for x in fq_paths]

            if not path_list:
                sys.exit("ERROR: No fastqs found for " + sample + ".") 
                          
            return path_list

        # Function to build merge command
        def _build_merge_cmd(path_list, merged_path):
            cmds = ""

            for fq_path in path_list:
                cmds += " " + fq_path

            cmds = "cat" + cmds + " > " + merged_path

            return cmds

        # Function to merge fastq files or create symlink
        def _merge_fastqs(sample, merged_name, raw_dir):

            # Merge fastqs for each read or create a symlink
            for read in ["_R1_", "_R2_", "_I1_"]:
                names = sample.split(",")

                # Create list of commands for creating symlinks
                if len(names) == 1:
                    path_list = _get_fq_paths(names[0], read, raw_dir)

                    cmd_list = []
                    for x in path_list:
                        if re.search("_[0-9]" + params.fq_info, x):
                            fastq_tail = re.search("_[0-9]" + params.fq_info, x).group(0)
                        else:
                            fastq_tail = re.search(params.fq_info, x).group(0)
                        merged_path = os.path.join(params.fq_dir, merged_name + fastq_tail)
                        cmd_list += ["ln -s " + x + " " + merged_path]

                # Create list of commands for merging fastqs
                else:
                    path_dict = {}

                    for name in names:
                        path_list = []
                        [path_list.append(x) for x in _get_fq_paths(name, read, raw_dir)]
                        path_dict[name] = path_list
                    path_dict
                    path_list = list(zip(path_dict[names[0]], path_dict[names[1]]))
                    cmd_list = []
                    for i in path_list:
                        if re.search("_[0-9]" + params.fq_info, i[0]):
                            fastq_tail = re.search("_[0-9]" + params.fq_info, i[0]).group(0)
                        else:
                            fastq_tail = re.search(params.fq_info, i[0]).group(0)                        
                        merged_path = os.path.join(params.fq_dir, merged_name + fastq_tail)

                        cmd_list.append(_build_merge_cmd(i, merged_path))

                for cmd in cmd_list:
                    subprocess.run(cmd, shell = True)

        # Find rna, adt, vdj, and data dir for the sample
        rna = params.rna_dict[wildcards.sample]
        adt = params.adt_dict[wildcards.sample]
        vdj_t = params.vdj_t_dict[wildcards.sample]
        vdj_b = params.vdj_b_dict[wildcards.sample]
        raw_dir = params.raw_data[wildcards.sample]
        print("data_dir")
        print(raw_dir)
        # Create symlinks for gene expression fastqs
        merged_names_gex = wildcards.sample + "_GEX"
        _merge_fastqs(rna, merged_names_gex, raw_dir)

        # Merge CITE-seq and cell hashing fastqs
        if adt:
            merged_names_adt = wildcards.sample + "_FB"

            _merge_fastqs(adt, merged_names_adt, raw_dir)

        # Create symlinks for VDJ fastqs
        if vdj_t:
            merged_names_vdj = wildcards.sample + "_VDJ_T"
            _merge_fastqs(vdj_t, merged_names_vdj, raw_dir)

        if vdj_b:
            merged_names_vdj = wildcards.sample + "_VDJ_B"
            _merge_fastqs(vdj_b, merged_names_vdj, raw_dir)

        # Write output file
        with open(output[0], "w") as out:
            out.write("done\n")

# Create sample csv ============================================================
# This rule creates a csv file used by cellranger count that contains the path 
# to the fastq directory, each fastq prefix, and the library type.

rule create_sample_csv:
    input:
        "{results}/snake_outs/{sample}_merge_fastqs_done.txt"
    output:
        "{results}/snake_outs/{sample}_csv_done.txt"
    params:
        results    = RESULTS,
        fq_dir     = FASTQ_DIR,
        gene_ref   = GENOME,
        adt_ref    = ADT_REF,
        vdj_ref    = VDJ_REF,
        fq_info    = FASTQ_INFO,
        rna_dict   = SAMPLE_DICT_RNA,
        adt_dict   = SAMPLE_DICT_ADT,
        vdj_t_dict = SAMPLE_DICT_VDJ_T,
        vdj_b_dict = SAMPLE_DICT_VDJ_B
    resources:
        slurm_extra=lambda wildcards: (
            f"--output={wildcards.results}/logs/cellranger_csv/{wildcards.sample}_cellranger.out "
            f"--error={wildcards.results}/logs/cellranger_csv/{wildcards.sample}_cellranger.err"
        )
    threads:
        1   
    run:
        # Function to create sample csv file for cellranger count
        def _create_sample_csv_count(sample_name, lib_type, sample_csv,
            fq_dir):
            fq_path = os.path.join(fq_dir, sample_name + "*.fastq.gz")
            fastqs  = glob.glob(fq_path)
            R1_fqs  = [x for x in fastqs if "_R1_" in x]

            # Trim fastq names
            R1_fqs = [os.path.basename(x) for x in R1_fqs]
            R1_fqs = [re.sub(params.fq_info, "", x) for x in R1_fqs]
            R1_fqs = set(R1_fqs)

            # Create sample csv
            if not os.path.isfile(sample_csv):
                with open(sample_csv, "w") as csv:
                    csv.write("fastqs,sample,library_type\n")

            with open(sample_csv, "a") as csv:
                for fq in R1_fqs:
                    csv.write("{},{},{}\n".format(fq_dir, fq, lib_type))

        def _create_sample_csv_multi(sample_csv, sample_names, fq_dir):
            with open(sample_csv, "w") as csv:
                csv.write("[gene-expression]\n")
                csv.write("reference," + params.gene_ref + "\n")
                if params.adt_dict[sample_names]:
                    csv.write("[feature]\n")
                    csv.write("reference," + params.adt_ref + "\n")
                csv.write("[vdj]\n")
                csv.write("reference," + params.vdj_ref + "\n")
                csv.write("[libraries]\n")
                csv.write("fastq_id,fastqs,feature_types\n")
                rna_fastqs = _get_fastqs(sample_names + "_GEX")
                for fastq in rna_fastqs:
                    csv.write("{},{},{}\n".format(fastq, fq_dir, "Gene Expression"))
                if params.adt_dict[sample_names]:
                    adt_fastqs = _get_fastqs(sample_names + "_FB")
                    for fastq in adt_fastqs:
                        csv.write("{},{},{}\n".format(fastq, fq_dir, "Antibody Capture"))
                vdj_t_fastqs = _get_fastqs(sample_names + "_VDJ_T")
                for fastq in vdj_t_fastqs:
                    csv.write("{},{},{}\n".format(fastq, fq_dir, "VDJ-T"))
                vdj_b_fastqs = _get_fastqs(sample_names + "_VDJ_B")
                for fastq in vdj_b_fastqs:
                    csv.write("{},{},{}\n".format(fastq, fq_dir, "VDJ-B"))

        def _get_fastqs(sample_id):
            fq_path = os.path.join(params.fq_dir, sample_id + "*.fastq.gz")
            fastqs  = glob.glob(fq_path)
            R1_fqs  = [x for x in fastqs if "_R1_" in x]

            # Trim fastq names
            R1_fqs = [os.path.basename(x) for x in R1_fqs]
            R1_fqs = [re.sub(params.fq_info, "", x) for x in R1_fqs]
            R1_fqs = set(R1_fqs)
            return(R1_fqs)

        if params.vdj_t_dict[wildcards.sample] or params.vdj_b_dict[wildcards.sample]:
            # Create sample csv file for cellranger multi
            sample_csv = os.path.join(params.results, wildcards.sample + "_multi.csv")
            if os.path.isfile(sample_csv):
                os.remove(sample_csv)
            _create_sample_csv_multi(sample_csv, wildcards.sample, params.fq_dir)
        else:
            # Create sample csv file for cellranger count
            sample_csv = os.path.join(params.results, wildcards.sample + "_count.csv")
            rna_id     = wildcards.sample + "_GEX"
        
            if os.path.isfile(sample_csv):
                os.remove(sample_csv)
            if params.adt_dict[wildcards.sample]:
                adt_id = wildcards.sample + "_FB"
                _create_sample_csv_count(adt_id, "Antibody Capture", sample_csv,
                    params.fq_dir)
    
            _create_sample_csv_count(rna_id, "Gene Expression", sample_csv,
                params.fq_dir)

        # Write output file
        with open(output[0], "w") as out:
            out.write("done\n")

# Run cellranger count =========================================================
# This rule runs cellranger count using csv files from create_sample_csv. If
# vdj samples are present it will run cellranger multi. If vdj samples are not
# present, it will run cellranger count

rule cellranger_count:
    input:
        "{results}/snake_outs/{sample}_csv_done.txt"
    output:
        "{results}/snake_outs/{sample}_count_done.txt"
    params:
        genome     = GENOME,
        max_jobs   = MAX_JOBS,
        barcodes   = ADT_REF,
        scratch    = RESULTS2,
        template   = CLUSTER_TEMPLATE,
        run_type   = _find_run_type,
        cellranger = CELLRANGER_PATH
    resources:
        slurm_extra=lambda wildcards: (
            f"--output={wildcards.results}/logs/cellranger/{wildcards.sample}_cellranger.out "
            f"--error={wildcards.results}/logs/cellranger/{wildcards.sample}_cellranger.err"
        )
    threads:
        10
    # singularity:
    #     config["CELLRANTER_CONTAINER"]
    shell:
        """
        echo $PATH
        cp {params.template} {params.scratch}
        
        # Run cellranger multi for vdj (CITE-seq) and gene expression
        if [ "{params.run_type}" == "VDJ" ]
        then
            sample_csv={wildcards.results}/{wildcards.sample}_multi.csv

            cd {params.scratch}

            {params.cellranger} multi \
                --id={wildcards.sample} \
                --csv=$sample_csv \
                --jobmode={params.template} \
                --maxjobs={params.max_jobs} \
                --create-bam true

        # Run cellranger count for CITE-seq and gene expression
        elif [ "{params.run_type}" == "CITE" ]
        then
            sample_csv={wildcards.results}/{wildcards.sample}_count.csv
            ab_csv={params.barcodes}

            cd {params.scratch}

            {params.cellranger} count \
                --id={wildcards.sample} \
                --libraries=$sample_csv \
                --feature-ref=$ab_csv \
                --transcriptome={params.genome} \
                --jobmode={params.template} \
                --maxjobs={params.max_jobs} \
                --create-bam true
        
        else
            sample_csv={wildcards.results}/{wildcards.sample}_count.csv

            cd {params.scratch}

            {params.cellranger} count \
                --id={wildcards.sample} \
                --libraries=$sample_csv \
                --transcriptome={params.genome} \
                --create-bam true \
                --jobmode={params.template} \
                --maxjobs={params.max_jobs}


        fi

        # Write output file
        touch {output}
        """

# Create multi csv =========================================================
# This rule creates a csv file to aggregate the counts from a multi run

rule create_aggr_csv:
    input:
        expand(
            "{results}/snake_outs/{sample}_count_done.txt",
            results = RESULTS, sample = SAMPLES
        )
    output:
        "{results}/snake_outs/{group}_csv_aggr_done.txt"
    params:
        results    = RESULTS,
        max_jobs   = MAX_JOBS,
        vdj_t_dict = SAMPLE_DICT_VDJ_T,
        vdj_b_dict = SAMPLE_DICT_VDJ_B,
        scratch    = RESULTS2
    resources:
        slurm_extra=lambda wildcards: (
            f"--output={wildcards.results}/logs/aggr_csv/{wildcards.group}_aggr_csv.out "
            f"--error={wildcards.results}/logs/aggr_csv/{wildcards.group}_aggr_csv.err"
        )
    threads:
        1   
    run:
        def _aggr_multi(use_samples, group_csv, result_dir):
            with open(group_csv, "w") as csv_file:
                csv_file.write("library_id,library_outs\n")
                for sample in use_samples:
                    out_dir = os.path.join(result_dir, sample, "outs")
                    csv_file.write("%s,%s\n" % (sample, out_dir))

        def _aggr_count(use_samples, group_csv, result_dir):
            with open(group_csv, "w") as csv_file:
                csv_file.write("library_id,molecule_h5\n")
                for sample in use_samples:
                    h5_file = os.path.join(result_dir, sample, "outs",
                        "molecule_info.h5")
                    csv_file.write("%s,%s\n" % (sample, h5_file))
        if(wildcards.group != "none"):
            use_samples = AGGR_GROUP[wildcards.group]
            group_csv = os.path.join(params.scratch, wildcards.group + "_aggr.csv")
            if params.vdj_t_dict[wildcards.sample] or params.vdj_t_dict[wildcards.sample]:
                _aggr_multi(use_samples, group_csv, params.scratch)
            else:
                _aggr_count(use_samples, group_csv, params.scratch)
        # Write output file
        with open(output[0], "w") as out:
            out.write("done\n")


rule run_aggr:
    input:
        "{results}/snake_outs/{group}_csv_aggr_done.txt"
    output:
        "{results}/snake_outs/{group}_cellranger_aggr_done.txt"
    params:
        csv      = "{results}/{group}_aggr.csv",
        max_jobs = MAX_JOBS,
        results  = RESULTS
    resources:
        slurm_extra=lambda wildcards: (
            f"--output={wildcards.results}/logs/aggr/{wildcards.group}_aggr.out "
            f"--error={wildcards.results}/logs/aggr/{wildcards.group}_aggr.err"
        )
    threads:
        1   
    run:
        if wildcards.group != "none":
            shell(
                '''
                cd {params.scratch}
                
                cellranger aggr \
                    --id={wildcards.group} \
                    --csv={params.csv}
                '''
                )
        # Write output file
        with open(output[0], "w") as out:
            out.write("done\n")


rule copy_files:
    input:
        "{results}/snake_outs/{sample}_count_done.txt"
    output:
        "{results}/snake_outs/{sample}_moved_files.txt"
    params:
        scratch    = RESULTS2,
        experiment = _find_experiment_type
    resources:
        slurm_extra=lambda wildcards: (
            f"--output={wildcards.results}/logs/copy/{wildcards.sample}_copy.out "
            f"--error={wildcards.results}/logs/copy/{wildcards.sample}_copy.err"
        )
    threads:
        1   
    shell:
        """
        echo {params.experiment}
        if [ {params.experiment} == "RNA" ]; then
            save_dir={wildcards.results}/{wildcards.sample}/outs
            mkdir -p $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/filtered_feature_bc_matrix $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/raw_feature_bc_matrix $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/web_summary.html $save_dir
        # Add in rules to copy other file types
        elif [ {params.experiment} == "VDJT_VDJB" ]; then
            save_dir={wildcards.results}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}
            full_save_dir=${{save_dir}}/count
            mkdir -p $full_save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/count/sample_filtered_feature_bc_matrix $full_save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/vdj_b $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/vdj_t $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/multi/count/raw_feature_bc_matrix $full_save_dir

            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/web_summary.html $save_dir
         # Add in rules to copy other file types
        elif [ {params.experiment} == "VDJT" ]; then
            save_dir={wildcards.results}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}
            full_save_dir=${{save_dir}}/count
            mkdir -p $full_save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/count/sample_filtered_feature_bc_matrix $full_save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/vdj_t $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/multi/count/raw_feature_bc_matrix $full_save_dir

            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/web_summary.html $save_dir
         # Add in rules to copy other file types
        elif [ {params.experiment} == "VDJB" ]; then
            save_dir={wildcards.results}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}
            full_save_dir=${{save_dir}}/count
            mkdir -p $full_save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/count/sample_filtered_feature_bc_matrix $full_save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/vdj_b $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/multi/count/raw_feature_bc_matrix $full_save_dir

            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/per_sample_outs/{wildcards.sample}/web_summary.html $save_dir
         # Add in rules to copy other file types
        elif [ {params.experiment} == "ADT" ]; then
            save_dir={wildcards.results}/{wildcards.sample}/outs
            mkdir -p $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/filtered_feature_bc_matrix $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/raw_feature_bc_matrix $save_dir
            rsync -avz --progress {params.scratch}/{wildcards.sample}/outs/web_summary.html $save_dir
        fi

        touch {output}
        """

